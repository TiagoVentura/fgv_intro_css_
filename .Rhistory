# informacao
nome <- source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish()
partido <-  source %>%
html_nodes(css=".partido") %>%
html_text()
biografia <- source %>%
html_nodes(css=".margintop11") %>%
html_text() %>%
paste0(., collapse = " ")
telefone <- source %>%
html_nodes(css=".margin_bottom_5+ p") %>%
html_text() %>%
paste0(., collapse = " ")
email <- source %>%
html_nodes(css="#formVisualizarPerfilDeputado p+ p") %>%
html_text() %>%
paste0(., collapse = " ")
# Combina tudo como um banco de dados
deputados <- tibble(nome, link, partido, biografia, telefone, email)
# Output
return(deputados)
# Desligando R um pouco para nao sobrecarregar os dados
Sys.sleep(sample(5:10, 1))
}
raspar_alerj <- function(url){
#source
source <- url %>% read_html()
# informacao
nome <- source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish()
partido <-  source %>%
html_nodes(css=".partido") %>%
html_text()
biografia <- source %>%
html_nodes(css=".margintop11") %>%
html_text() %>%
paste0(., collapse = " ")
telefone <- source %>%
html_nodes(css=".margin_bottom_5+ p") %>%
html_text() %>%
paste0(., collapse = " ")
email <- source %>%
html_nodes(css="#formVisualizarPerfilDeputado p+ p") %>%
html_text() %>%
paste0(., collapse = " ")
# Combina tudo como um banco de dados
deputados <- tibble(nome, link, partido, biografia, telefone, email)
# Output
return(deputados)
# Desligando R um pouco para nao sobrecarregar os dados
Sys.sleep(sample(1:3, 1))
}
# Converter os links para uma lista.
lista_links <- as.list(dados$links)
dados <- map(lista_links[[10]], raspar_alerj)
for(i in 1:length(lista_links)){
raspar_alerj(lista_links[[i]])
print(i)
}
dados <- map(lista_links, raspar_alerj)
# Combine tudo como um banco de dados
dados_alerj <- bind_rows(dados)
# Vamos ver
kable(dados_alerj) %>%
kable_styling() %>%
scroll_box(width = "100%", height = "200px")
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
# url
link <- dados$links[[1]][1:10]
blogdown::serve_site()
xaringan::inf_mr()
glimpse(tabelas)
tabelas[[1]]
tabela_limpa
xaringan::inf_mr()
xaringan::inf_mr()
blogdown::build_site()
blogdown::serve_site()
xaringan::inf_mr()
xaringan::inf_mr()
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
xaringan::inf_mr()
# library
library("tidyverse")
library("purrr")
library("rvest")
library("stringr")
library("kableExtra")
set.seed(123)
x <- rnorm(1000)y <- 1 + 2*x + 1*xˆ2 - .5*xˆ3  + rnorm(1000,sd=3)
dados  <- tibble(y,x) %>%mutate(color = ifelse(x>=0,"Pos-Tratamento","Pre-Tratamento"),y = ifelse(x>=0,y+5,y),color = fct_rev(color))
set.seed(123)
x <- rnorm(1000)
y <- 1 + 2*x + 1*xˆ2 - .5*xˆ3  + rnorm(1000,sd=3)
dados  <- tibble(y,x) %>%mutate(color = ifelse(x>=0,"Pos-Tratamento","Pre-Tratamento"),y = ifelse(x>=0,y+5,y),color = fct_rev(color))
x <- rnorm(1000)
y <- 1 + 2*x + 1*xˆ2 - .5*xˆ3  + rnorm(1000,sd=3)
y <- 1 + 2*x + 1*x^2 - .5*x^3  + rnorm(1000,sd=3)
dados  <- tibble(y,x) %>%mutate(color = ifelse(x>=0,"Pos-Tratamento","Pre-Tratamento"),y = ifelse(x>=0,y+5,y),color = fct_rev(color))
ggplot(data=dados,aes(y=y,x=x)) +
geom_point(aes(alpha=color, shape=color, color=color)) +
geom_smooth(color="black",size=1, alpha=.6)
ggplot(data=dados,aes(y=y,x=x,
alpha=color,
shape=color,
color=color)) +
geom_point() +
geom_smooth()
ggplot(data=dados,aes(y=y,x=x,
alpha=color,
shape=color,
color=color,
linetype=color)) +
geom_point() +
geom_smooth()
ggplot(dados, aes(x,y,color=color, pch=color)) +
geom_point(size=3,alpha=.5) +
geom_smooth() +
theme_minimal() +
scale_color_manual(values=c("grey30","steelblue")) +
labs(color="",pch="",x="X",y="Y",
title="Regressão Discontinuada",
caption = "\nSource: Eric Dunford") +
theme(legend.position = "bottom",
plot.title = element_text(size=20,family='serif'),
text = element_text(size=20,family='serif'),
axis.text = element_text(size=18,family='serif'),
axis.title = element_text(size=18,family='serif'))
ggplot(data=dados,aes(y=y,x=x)) +
geom_point() +
geom_smooth(aes(alpha=color,
shape=color,
color=color,
linetype=color))
ggplot(dados, aes(x,y,color=color, pch=color)) +
geom_point(size=3,alpha=.5) +
geom_smooth()
ggplot(data=dados,aes(y=y,x=x)) +
geom_point() +
geom_smooth(aes(alpha=color,
shape=color,
color=color,
linetype=color))
ggplot(data=dados,aes(y=y,x=x)) +
geom_point(aes(alpha=color,
shape=color,
color=color,
linetype=color)) +
geom_smooth(aes(alpha=color,
shape=color,
color=color,
linetype=color))
ggplot(dados, aes(x,y,color=color)) +
geom_point(size=3,alpha=.5) +
geom_smooth()
# Raspagem de Dados
minha_url <- "https://pt.wikipedia.org/wiki/Lista_de_munic%C3%ADpios_fronteiri%C3%A7os_do_Brasil"
minha_url
# Passo 2
source <- rvest::read_html(minha_url)
# Passo 2
source <- read_html(minha_url)
?read_html
# Passo 2
source <- read_html(minha_url)
class(source)
class(source)
source
tabelas <- source %>%
rvest::html_table()
# Olhem a magica
tabelas
# Olhem a magica
str(tabelas)
view(tabelas[[3]])
# Olhem a magica
tabela_limpa <- tabelas[[3]] %>%
# Converte para um banco de dados mais bonito
as.tibble() %>%
# Cria Duas novas Colunas
mutate(city = Município,
uf_name = Estado) %>%
select(city, uf_name) %>%
# consertar o enconding
mutate(city = str_sub(city,5),
city = str_replace(city, pattern="- ", ""),
city =  str_trim(city),
city_key = stringi::stri_trans_general(city, "Latin-ASCII"),
city_key= str_replace_all(city_key, " ", ""),
city_key=str_to_lower(city_key))
tabela_limpa
# Passo 1
minha_url <- "http://www.alerj.rj.gov.br/Deputados/QuemSao"
minha_url
# Passo 2
nomes <- read_html(minha_url)
nomes
# Passo
nomes_limpos <- nomes %>%
html_nodes(css=".nome a")
nomes_limpos
# Passo
nomes_limpos <- nomes %>%
html_nodes(css=".nome a") %>%
html_text()
nomes_limpos
# Tudo junto
links <- read_html(minha_url) %>%
html_nodes(css=".nome a") %>%
html_attr("href")
links
links <- paste0("http://www.alerj.rj.gov.br/", links)
links
# Criar um banco de dados.
dados <- tibble(nomes=nomes,
links=links)
# Criar um banco de dados.
dados <- tibble(nomes=nomes_limpos,
links=links)
dados
browseURL(dados$links[[1]])
browseURL(dados$links[[10]])
browseURL(dados$links[[70]])
nomes_limpos <- nomes %>%
html_nodes(css=".nome a")
nomes_limpos
omes %>%
html_nodes(css=".nome a") %>%
html_text()
nomes %>%
html_nodes(css=".nome a") %>%
html_text()
read_html(minha_url) %>%
html_nodes(css=".nome a") %>%
html_attr("href")
dados$links
link <- dados$links[[1]]
link
#source
source <- link %>%
read_html()
source
browseURL(link)
browseURL(link)
source %>%
html_nodes(css=".paginacao_deputados .titulo")
source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text()
source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n")
source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish()
nome <- source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish() %>%
str_to_title()
nome
source %>%
html_nodes(css=".partido") %>%
html_text()
partido <-  source %>%
html_nodes(css=".partido") %>%
html_text()
source %>%
html_nodes(css=".margintop11") %>%
html_text()
biografia <- source %>%
html_nodes(css=".margintop11") %>%
html_text() %>%
paste0(., collapse = " ")
biografia
telefone <- source %>%
html_nodes(css=".margin_bottom_5+ p") %>%
html_text()
telefone
email <- source %>%
html_nodes(css="#formVisualizarPerfilDeputado p+ p") %>%
html_text()
deputados <- tibble(nome, link, partido, biografia, telefone, email)
deputados
# Chunk 1: setup
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, error = FALSE, cache = TRUE,
out.width = "50%")
# Chunk 2: knitr_init
library(knitr)
library(rmdformats)
## Global options
opts_chunk$set(cache=TRUE,
prompt=FALSE,
comment=NA,
message=FALSE,
warning=FALSE)
# Chunk 5
knitr::include_graphics("https://media.giphy.com/media/cCbf4ryl0UQWBwOLiQ/giphy.gif")
# Chunk 7
minha_url <- "https://pt.wikipedia.org/wiki/Lista_de_munic%C3%ADpios_fronteiri%C3%A7os_do_Brasil"
# Chunk 8
source <- read_html(minha_url)
# Chunk 9
class(source) # XML=HTML
# Chunk 10
tabelas <- source %>%
rvest::html_table()
# Chunk 11
tabelas[[3]]
# Chunk 12
tabela_limpa <- tabelas[[3]] %>%
# Converte para um banco de dados mais bonito
as.tibble() %>%
# Cria Duas novas Colunas
mutate(city = Município,
uf_name = Estado) %>%
select(city, uf_name) %>%
# consertar o enconding
mutate(city = str_sub(city,5),
city = str_replace(city, pattern="- ", ""),
city =  str_trim(city),
city_key = stringi::stri_trans_general(city, "Latin-ASCII"),
city_key= str_replace_all(city_key, " ", ""),
city_key=str_to_lower(city_key))
tabela_limpa
# Chunk 13
# Coleta de todos os nomes
minha_url <- "http://www.alerj.rj.gov.br/Deputados/QuemSao"
nomes <- read_html(minha_url) %>%
html_nodes(css=".nome a") %>%
html_text() #<<
# Limpa os nomes
nomes_limpos <- nomes %>%
str_to_title()
nomes_limpos
# Chunk 14
links <- read_html(minha_url) %>%
html_nodes(css=".nome a") %>%
html_attr("href") #<<
# Combina os links com a estrutura básica da página da UERJ.
links <- paste0("http://www.alerj.rj.gov.br/", links)
# Criar um banco de dados.
dados <- tibble(nomes=nomes,
links=links)
# Chunk 15
# url
link <- dados$links[[1]]
#source
source <- link %>% read_html()
# informacao
nome <- source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish()
partido <-  source %>%
html_nodes(css=".partido") %>%
html_text()
biografia <- source %>%
html_nodes(css=".margintop11") %>%
html_text() %>%
paste0(., collapse = " ")
telefone <- source %>%
html_nodes(css=".margin_bottom_5+ p") %>%
html_text()
email <- source %>%
html_nodes(css="#formVisualizarPerfilDeputado p+ p") %>%
html_text()
# Combina tudo como um banco de dados
deputados <- tibble(nome, link, partido, biografia, telefone, email)
# Chunk 16
deputados
# Chunk 17
raspar_alerj <- function(url){
#source
source <- url %>% read_html()
# informacao
nome <- source %>%
html_nodes(css=".paginacao_deputados .titulo") %>%
html_text() %>%
str_remove_all(., "\\r|\\n") %>%
str_trim() %>%
str_squish()
partido <-  source %>%
html_nodes(css=".partido") %>%
html_text()
biografia <- source %>%
html_nodes(css=".margintop11") %>%
html_text() %>%
paste0(., collapse = " ")
telefone <- source %>%
html_nodes(css=".margin_bottom_5+ p") %>%
html_text() %>%
paste0(., collapse = " ")
email <- source %>%
html_nodes(css="#formVisualizarPerfilDeputado p+ p") %>%
html_text() %>%
paste0(., collapse = " ")
# Combina tudo como um banco de dados
deputados <- tibble(nome, link, partido, biografia, telefone, email)
# Output
return(deputados)
# Desligando R um pouco para nao sobrecarregar os dados
Sys.sleep(sample(1:3, 1))
}
raspar_alerj(dados$links[[5]])
xaringan::inf_mr()
url <- "https://www.latinnews.com/component/k2/itemlist/category/33.html?archive=true&archive_id=33&period=2020"
years <- 2003:2004
url_bind <- paste0("https://www.latinnews.com/component/k2/itemlist/category/33.html?archive=true&archive_id=33&period=", years)
url <- url_bind[1]
# Get links from the url
to_get_url <- function(url){
text <- url %>%
read_html() %>%
html_nodes(".archive_item") %>%
html_text()
link <- url %>%
read_html() %>%
html_nodes(".archive_item") %>%
html_attr("href") %>%
paste0("https://www.latinnews.com/", .)
link <- link[1:5]
text <- text[1:5]
list_abstracts <- map_chr(link, to_get_links) %>%
map_chr(~str_replace_all(.x, "[\t\r\n]" , ""))
outputs <- list(text, link, list_abstracts) %>%
set_names(., nm=c("text", "link", "abstracts"))
return(outputs)
}
to_get_links<- function(link){
print("Collected")
Sys.sleep(runif(1, 0,5))
abstract <- link %>%
read_html() %>%
html_nodes(".itemFullText") %>%
html_text() %>%
paste0(., collapse = " ")
return(abstract)
}
url_bind[[1]]
url_bind
res <- to_get_url(url_bind[[1]]) %>%
as_tibble() %>%
mutate(years=years[[1]])
res
View(res)
res$abstracts[[1]]
res$abstracts[[3]]
blogdown::serve_site()
library(tidyverse)
library(purrr)
library(rvest)
library(stringr)
library(kableExtra)
minha_url2 <- "https://www.al.sp.gov.br/deputado/lista/"
nomes <- read_html(minha_url2) %>%
html_nodes(css=".link_canais") %>%
html_text()
nomes
# Limpa os nomes
nomes_limpos <- nomes %>%
str_to_title()
links <- read_html(minha_url2) %>%
html_nodes(css=".link_canais") %>%
html_attr("href")
links <- paste0("https://www.al.sp.gov.br/deputado/lista/", links)
links
links <- paste0("https://www.al.sp.gov.br/deputado/", links)
links
links <- read_html(minha_url2) %>%
html_nodes(css=".link_canais") %>%
html_attr("href")
links <- paste0("https://www.al.sp.gov.br/deputado/", links)
links
browseURL(links[1])
links
links <- read_html(minha_url2) %>%
html_nodes(css=".link_canais") %>%
html_attr("href")
links
links <- paste0("https://www.al.sp.gov.br/", links)
browseURL(links[1])
